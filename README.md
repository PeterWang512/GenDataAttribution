## Evaluating Data Attribution for Text-to-Image Models
[**Project**](https://peterwang512.github.io/GenDataAttribution/) | [**Paper**](https://arxiv.org/abs/2306.09345)


<p align="center">
 <img src="images/thumbnail.jpg" width="800px"/>
</p>


(a) We create an visual data attribution dataset by taking a pretrained generative model and tuning it toward an exemplar image (or images) using "customization". This produces a set of synthesized images that are computationally influenced by the exemplar by construction.
(b) Given the dataset, we can evaluate data attribution approaches by how high they rank the exemplar relative to other training images. Furthermore, using our dataset, we tune representations toward attribution and estimate the probability a given image was an exemplar.
<br><br><br>

[Sheng-Yu Wang](https://peterwang512.github.io/)<sup>1</sup>, [Alexei A. Efros](https://people.eecs.berkeley.edu/~efros/)<sup>2</sup>, [Jun-Yan Zhu](https://cs.cmu.edu/~junyanz)<sup>1</sup>, [Richard Zhang](http://richzhang.github.io/)<sup>3</sup>.
<br> Carnegie Mellon University<sup>1</sup>, UC Berkeley<sup>2</sup>, Adobe Research<sup>2</sup>
<br>In [ICCV](https://arxiv.org/abs/2306.09345), 2023.


## Setup
### Set up conda environment
```bash
conda env create -f environment.yaml
conda activate gen-attr
```

### Download model weights and data
```bash
# Download precomputed features of 1M LAION images
bash feats/download_laion_feats.sh

# Download jpeg-ed 1M LAION images for visualization
bash dataset/download_dataset.sh laion_jpeg

# Download pretrained models
bash weights/download_weights.sh
```

## Quick Start (Interactive Demo)
<p align="center">
<img src="images/demo.jpg" width="800px"/>
</p>
We estimate training data influence by learned feature similarity. This demo takes in a Stable-Diffusion-generated image as query, and output influence scores from the 1M subset of LAION images. To launch the demo, run:

```bash
streamlit run streamlit_demo.py
```

## Dataset
We release our testset for evaluation. To download the dataset:
```bash
# Download the exemplar real images
bash dataset/download_dataset.sh exemplar

# Download the testset portion of images synthesized from Custom Diffusion
bash dataset/download_dataset.sh testset

# (Optional, can download precomputed features instead!)
# Download the uncompressed 1M LAION subset in pngs
bash dataset/download_dataset.sh laion
```
The dataset is structured as follows:
```
dataset
├── exemplar
│   ├── artchive
│   ├── bamfg
│   └── imagenet
├── synth
│   ├── artchive
│   ├── bamfg
│   └── imagenet
├── laion_subset
└── json
    ├──test_artchive.json
    ├──test_bamfg.json
    ├──...
```
All exemplar images are stored in `dataset/exemplar`, all synthesized images are stored in `dataset/synth`, and 1M laion images in pngs are stored in `dataset/laion_subset`. The JSON files in `dataset/json` specify the train/val/test splits, including different test cases, and serve as ground truth labels. Each entry inside a JSON file is a unique fine-tuned model. An entry also records the exemplar image(s) used for finetuning and the synthesized images generated by the model. We have four test cases: `test_artchive.json`, `test_bamfg.json`, `test_observed_imagenet.json`, and `test_unobserved_imagenet.json`.

#### (We will release the train and validation set in the near future!)

## Evaluation
After the testset, precomputed LAION features, and pretrained weights are downloaded, we can precompute the features from the testset by running `extract_feat.py`, and then evaluate the performance by running `eval.py`. Below are the bash scripts that runs the evaluation in batches:
```bash
# precompute all features from the testset
bash scripts/preprocess_feats.sh

# run evaluation in batches
bash scripts/run_eval.sh
```
The metrics are stored in `.pkl` files in `results`. Currently, the script runs each command sequentially. Please feel free to modify it to run the commands in parallel. The following command will parse the `.pkl` files into tables stored as `.csv` files:
```bash
python results_to_csv.py
```

## Citation
```
@inproceedings{wang2023evaluating,
  title={Evaluating Data Attribution for Text-to-Image Models},
  author={Wang, Sheng-Yu and Efros, Alexei A. and Zhu, Jun-Yan and Zhang, Richard},
  booktitle={ICCV},
  year={2023}
}
```

## Acknowledgement
We thank Aaron Hertzmann for reading over an earlier draft and for insightful feedback. We thank colleagues in Adobe Research, including Eli Shechtman, Oliver Wang, Nick Kolkin, Taesung Park, John Collomosse, and Sylvain Paris, along with Alex Li and Yonglong Tian for helpful discussion. We appreciate Nupur Kumari for guidance with Custom Diffusion training, Ruihan Gao for proof-reading the draft, Alex Li for pointers to extract Stable Diffusion features, and Dan Ruta for help with the BAM-FG dataset. We thank Bryan Russell for pandemic hiking and brainstorming. This work started when SYW was an Adobe intern and was supported in part by an Adobe gift and J.P. Morgan Chase Faculty Research Award.
